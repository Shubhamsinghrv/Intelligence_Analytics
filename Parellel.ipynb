{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d90c401-f0ad-45ba-80c0-e91a0bf7ed89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:38301' processes=8 threads=16, memory=14.90 GiB>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 11:26:51,543 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 2b0dfe73674bba35f521f35f3cbbe7f4 initialized by task ('shuffle-transfer-2b0dfe73674bba35f521f35f3cbbe7f4', 15) executed on worker tcp://127.0.0.1:40993\n",
      "2025-02-07 11:26:52,955 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 2b0dfe73674bba35f521f35f3cbbe7f4 deactivated due to stimulus 'task-finished-1738927612.9489005'\n",
      "2025-02-07 11:26:59,386 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle f34afbf1aaf39b7233c9dc4ba4edb8db initialized by task ('shuffle-transfer-f34afbf1aaf39b7233c9dc4ba4edb8db', 0) executed on worker tcp://127.0.0.1:46781\n",
      "2025-02-07 11:27:00,159 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle f34afbf1aaf39b7233c9dc4ba4edb8db deactivated due to stimulus 'task-finished-1738927620.157186'\n",
      "2025-02-07 11:27:06,989 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 9ad915b5eb3c1cc044da11b5c9db8748 initialized by task ('shuffle-transfer-9ad915b5eb3c1cc044da11b5c9db8748', 0) executed on worker tcp://127.0.0.1:37739\n",
      "2025-02-07 11:27:08,484 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 9ad915b5eb3c1cc044da11b5c9db8748 deactivated due to stimulus 'task-finished-1738927628.4824064'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Create a Dask client with 8 workers and 2 threads per worker\n",
    "client = Client(n_workers=8, threads_per_worker=2, memory_limit='2GB')\n",
    "\n",
    "# Print out the details of the Dask cluster\n",
    "print(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbba54ce-ff4a-4cf6-8426-f36af98b5924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.stats import chi2\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeRegressor, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier, AdaBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Suppress all warnings\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73476bc-c98c-4d59-b061-1cc57b35199a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BigQuery and Storage clients\n",
    "storage_client = storage.Client()\n",
    "\n",
    "print(\"Authentication successful!\")\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec42c57-baa2-456a-bfed-e9cd65abf0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import gcsfs\n",
    "\n",
    "\n",
    "\n",
    "# Load CSV into Dask DataFrame\n",
    "df = dd.read_csv(\n",
    "    \"gs://samplebucketvrs/Intelligence Analytics/og4/2019.csv\",\n",
    "    blocksize=\"50MB\",\n",
    "    assume_missing=True  # Converts int64 to float64 if missing values exist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995ea98b-7112-4cd4-bef8-8eaa4d88f44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check Data\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f2a8fe-d344-4714-924a-492179166b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Describe dataset (computes statistics on a sample)\n",
    "#print(df.describe().compute())\n",
    "\n",
    "# Drop 'id' column if it exists\n",
    "columns_to_drop = [col for col in df.columns if col.lower() == 'id']\n",
    "if columns_to_drop:\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Convert 'Date' column to datetime using Dask\n",
    "df['FL_DATE'] = df['FL_DATE'].map_partitions(pd.to_datetime, meta=('FL_DATE', 'datetime64[ns]'))\n",
    "\n",
    "# Verify changes\n",
    "dtypes = df.dtypes\n",
    "#print(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06b67c6-c8e9-41fb-97a1-741c2ddbd4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 20\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f5d629-a3c0-466f-a803-d323ce485246",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-selected method: iqr\n",
      "Removing 0.00% outliers from column 'OP_CARRIER_FL_NUM'.\n",
      "Removing 0.00% outliers from column 'DEP_TIME'.\n",
      "Applying Winsorization to column 'DEP_DELAY' with 9.50% outliers.\n",
      "Removing 3.93% outliers from column 'TAXI_OUT'.\n",
      "Removing 0.00% outliers from column 'WHEELS_OFF'.\n",
      "Removing 0.00% outliers from column 'WHEELS_ON'.\n",
      "Applying Winsorization to column 'TAXI_IN' with 6.19% outliers.\n",
      "Removing 0.00% outliers from column 'ARR_TIME'.\n",
      "Applying Winsorization to column 'ARR_DELAY' with 6.83% outliers.\n",
      "Removing 4.92% outliers from column 'AIR_TIME'.\n",
      "Applying Winsorization to column 'DISTANCE' with 5.70% outliers.\n",
      "Removing 1.77% outliers from column 'CARRIER_DELAY'.\n",
      "Removing 1.11% outliers from column 'WEATHER_DELAY'.\n",
      "Removing 1.28% outliers from column 'NAS_DELAY'.\n",
      "Removing 0.06% outliers from column 'SECURITY_DELAY'.\n",
      "Removing 1.43% outliers from column 'LATE_AIRCRAFT_DELAY'.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import normaltest\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "def detect_outliers_dask(df, method=None, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detects outliers using IQR or Z-score, with an option to auto-select the best method.\n",
    "    \"\"\"\n",
    "    outlier_percentages = {}\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    if method is None:\n",
    "        # Take a small sample (10%) efficiently using random_split\n",
    "        sample_frac = 0.1\n",
    "        df_sample, _ = df.random_split([sample_frac, 1 - sample_frac])\n",
    "        df_sample = df_sample.compute()  # Convert sample to Pandas DataFrame\n",
    "\n",
    "        normality_pvals = df_sample[numeric_cols].apply(lambda x: normaltest(x.dropna())[1])\n",
    "\n",
    "        if (normality_pvals > 0.05).all():  \n",
    "            method = \"zscore\"  # If p > 0.05, assume normal distribution\n",
    "        else:\n",
    "            method = \"iqr\"  # Otherwise, assume non-normal distribution\n",
    "\n",
    "        print(f\"Auto-selected method: {method}\")\n",
    "\n",
    "    # Now, use IQR or Z-score as before\n",
    "    if method == \"iqr\":\n",
    "        for col in numeric_cols:\n",
    "            quantiles = df[col].quantile([0.25, 0.75]).compute()\n",
    "            Q1, Q3 = quantiles.loc[0.25], quantiles.loc[0.75]\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "            mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            outlier_count = mask.sum().compute()\n",
    "            total_rows = len(df)\n",
    "            outlier_percentages[col] = (outlier_count / total_rows) * 100\n",
    "\n",
    "    elif method == \"zscore\":\n",
    "        for col in numeric_cols:\n",
    "            mean, std = df[col].mean().compute(), df[col].std().compute()\n",
    "            mask = abs((df[col] - mean) / std) > threshold\n",
    "            outlier_count = mask.sum().compute()\n",
    "            total_rows = len(df)\n",
    "            outlier_percentages[col] = (outlier_count / total_rows) * 100\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'iqr' or 'zscore'.\")\n",
    "\n",
    "    return outlier_percentages\n",
    "\n",
    "# Assuming df is already defined as a Dask DataFrame\n",
    "outlier_percentages = detect_outliers_dask(df, method=None)\n",
    "\n",
    "def clean_or_winsorize_dask(df, outlier_percentages, threshold=5):\n",
    "    \"\"\"\n",
    "    Cleans or applies Winsorization based on outlier percentage.\n",
    "    \"\"\"\n",
    "    numeric_cols = list(outlier_percentages.keys())\n",
    "\n",
    "    # Compute IQR bounds once for all columns\n",
    "    stats = df[numeric_cols].quantile([0.25, 0.75]).compute()\n",
    "    Q1, Q3 = stats.loc[0.25], stats.loc[0.75]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "    df_out = df.copy()  # Create a copy to avoid modifying the original\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if outlier_percentages[col] <= threshold:\n",
    "            print(f\"Removing {outlier_percentages[col]:.2f}% outliers from column '{col}'.\")\n",
    "            df_out = df_out.assign(**{col: df[col].where((df[col] >= lower_bound[col]) & (df[col] <= upper_bound[col]))})\n",
    "        else:\n",
    "            print(f\"Applying Winsorization to column '{col}' with {outlier_percentages[col]:.2f}% outliers.\")\n",
    "            df_out = df_out.assign(**{col: df[col].map_partitions(lambda x: winsorize(x, limits=(0.05, 0.05)), meta=(col, \"float64\"))})\n",
    "\n",
    "    return df_out\n",
    "\n",
    "# Clean or Winsorize Data\n",
    "df_cleaned = clean_or_winsorize_dask(df, outlier_percentages, threshold=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937a813-a918-4e8d-a127-c0434bf2904c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed median for OP_CARRIER_FL_NUM: 2296.0\n",
      "Imputed median for DEP_TIME: 1336.0\n",
      "Imputed median for DEP_DELAY: -1.0\n",
      "Imputed median for TAXI_OUT: 15.0\n",
      "Imputed median for WHEELS_OFF: 1350.0\n",
      "Imputed median for WHEELS_ON: 1515.0\n",
      "Imputed median for TAXI_IN: 6.0\n",
      "Imputed median for ARR_TIME: 1518.0\n",
      "Imputed median for ARR_DELAY: -4.0\n",
      "Imputed median for AIR_TIME: 96.0\n",
      "Imputed median for DISTANCE: 660.0\n",
      "Imputed median for CARRIER_DELAY: 3.0\n",
      "Imputed median for WEATHER_DELAY: 0.0\n",
      "Imputed median for NAS_DELAY: 6.0\n",
      "Imputed median for SECURITY_DELAY: 0.0\n",
      "Imputed median for LATE_AIRCRAFT_DELAY: 7.0\n",
      "Applied Label Encoding to OP_UNIQUE_CARRIER\n",
      "Applied Label Encoding to ORIGIN\n",
      "Applied Label Encoding to DEST\n",
      "FL_DATE                datetime64[ns]\n",
      "OP_UNIQUE_CARRIER               int64\n",
      "OP_CARRIER_FL_NUM             float64\n",
      "ORIGIN                          int64\n",
      "DEST                            int64\n",
      "DEP_TIME                      float64\n",
      "DEP_DELAY                     float64\n",
      "TAXI_OUT                      float64\n",
      "WHEELS_OFF                    float64\n",
      "WHEELS_ON                     float64\n",
      "TAXI_IN                       float64\n",
      "ARR_TIME                      float64\n",
      "ARR_DELAY                     float64\n",
      "AIR_TIME                      float64\n",
      "DISTANCE                      float64\n",
      "CARRIER_DELAY                 float64\n",
      "WEATHER_DELAY                 float64\n",
      "NAS_DELAY                     float64\n",
      "SECURITY_DELAY                float64\n",
      "LATE_AIRCRAFT_DELAY           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import LabelEncoder\n",
    "from dask import delayed\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\",\"string\"]).columns\n",
    "\n",
    "# ✅ Impute missing values for numerical columns using median\n",
    "for col in numerical_cols:\n",
    "    median_value = df[col].median_approximate().compute()  # Use approximate median for efficiency\n",
    "    df[col] = df[col].fillna(median_value)\n",
    "    print(f\"Imputed median for {col}: {median_value}\")\n",
    "\n",
    "# ✅ Encode categorical variables\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique().compute()  # Get number of unique values\n",
    "    \n",
    "    if unique_count <= 10:\n",
    "        # **One-Hot Encoding** for low-cardinality categorical columns\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        \n",
    "        # Apply One-Hot Encoding using map_partitions\n",
    "        df = df.map_partitions(\n",
    "            lambda d: d.join(\n",
    "                pd.DataFrame(\n",
    "                    encoder.fit_transform(d[[col]]),\n",
    "                    index=d.index,\n",
    "                    columns=[f\"{col}_{i}\" for i in range(unique_count)]\n",
    "                )\n",
    "            ), \n",
    "            meta=df\n",
    "        )\n",
    "        \n",
    "        df = df.drop(columns=[col])  # Drop original column after encoding\n",
    "        print(f\"Applied One-Hot Encoding to {col}\")\n",
    "\n",
    "    else:\n",
    "        # **Label Encoding** for high-cardinality categorical columns\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = df[col].map_partitions(lambda x: encoder.fit_transform(x), meta=(col, 'int64'))\n",
    "        print(f\"Applied Label Encoding to {col}\")\n",
    "\n",
    "# ✅ Check the transformed DataFrame\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6022b-d7a9-476d-b431-b9cb0d123a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f65f7-2ab6-4b05-98b3-4d04ac10e658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data Sampled\n",
      "✅ Data Scaled\n",
      "✅ Data Converted to NumPy Successfully!\n",
      "✅ Regression Model Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from joblib import parallel_backend  # Enables Dask parallelism in Scikit-learn\n",
    "\n",
    "# ✅ Sample 10% of the data\n",
    "sampled_df = df.sample(frac=0.1, random_state=42)\n",
    "print(\"✅ Data Sampled\")\n",
    "\n",
    "# ✅ Separate features (X) and target (y)\n",
    "target_col = \"DEP_DELAY\"\n",
    "y = sampled_df[target_col]\n",
    "X = sampled_df.drop(columns=[target_col, \"FL_DATE\"])\n",
    "\n",
    "# ✅ Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Data Scaled\")\n",
    "\n",
    "# ✅ Train/Test Split (remains Dask arrays)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Compute in **small batches** to avoid memory overload\n",
    "batch_size = 100000  # Adjust based on available memory\n",
    "\n",
    "def batch_compute(dask_array, batch_size):\n",
    "    \"\"\"Convert Dask array to NumPy in small chunks to avoid memory overflow.\"\"\"\n",
    "    np_array = dask_array.to_dask_array(lengths=True)  # Convert to Dask Array\n",
    "    n_rows = np_array.shape[0]\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, n_rows, batch_size):\n",
    "        results.append(np_array[i : i + batch_size].compute())  # Compute in chunks\n",
    "\n",
    "    return np.vstack(results) if results[0].ndim > 1 else np.hstack(results)\n",
    "\n",
    "# ✅ Convert X_train, X_test, y_train, y_test in small chunks\n",
    "X_train_np = batch_compute(X_train, batch_size)\n",
    "X_test_np = batch_compute(X_test, batch_size)\n",
    "y_train_np = batch_compute(y_train, batch_size)\n",
    "y_test_np = batch_compute(y_test, batch_size)\n",
    "\n",
    "print(\"✅ Data Converted to NumPy Successfully!\")\n",
    "\n",
    "# ✅ Determine task type & use Scikit-learn models with Dask backend\n",
    "if np.unique(y_train_np).size <= 10:\n",
    "    model = ExtraTreesClassifier(n_jobs=-1, random_state=42, n_estimators=10)\n",
    "    task_type = \"Classification\"\n",
    "else:\n",
    "    model = ExtraTreesRegressor(n_jobs=-1, random_state=42, n_estimators=10)\n",
    "    task_type = \"Regression\"\n",
    "\n",
    "# ✅ Train the model using Dask's parallel backend\n",
    "with parallel_backend(\"dask\"):\n",
    "    model.fit(X_train_np, y_train_np)\n",
    "\n",
    "print(f\"✅ {task_type} Model Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ebfec-320d-4f71-8db4-627aea6e3558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "def select_top_features_dask(model, X, min_features=5):\n",
    "    # Check if the model supports feature importances\n",
    "    if not hasattr(model, \"feature_importances_\"):\n",
    "        raise ValueError(\"Model does not support feature importances.\")\n",
    "    \n",
    "    # Ensure X is a Dask DataFrame\n",
    "    if not isinstance(X, (dd.DataFrame, pd.DataFrame)):\n",
    "        raise ValueError(\"X must be a pandas or Dask DataFrame.\")\n",
    "\n",
    "    # Convert to Dask if it is a pandas DataFrame for consistency\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = dd.from_pandas(X, npartitions=1)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame with features and their importance scores\n",
    "    importance_df = dd.from_pandas(pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True), npartitions=1)\n",
    "    \n",
    "    # Debugging statement\n",
    "    print(importance_df.compute())  # Ensure we see the DataFrame\n",
    "\n",
    "    total_features = len(importance_df)\n",
    "\n",
    "    # Dynamically calculate the number of features to select\n",
    "    additional_features = math.ceil(0.6 * total_features)\n",
    "    selected_features_count = min(len(X.columns), max(min_features, additional_features + min_features))\n",
    "\n",
    "    # Select top features based on importance\n",
    "    selected_features_df = importance_df.head(selected_features_count).compute()\n",
    "    selected_features = selected_features_df['Feature'].tolist()\n",
    "    \n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f147a-1169-4de4-a186-33d3d5207a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimportance_df\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'importance_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e99d29-c7d5-44cb-a4ef-eb1da23013df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.multiprocessing import Pool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier, ExtraTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "def select_top_features(model, X, min_features=5):\n",
    "    \"\"\"Selects top features based on model feature importances.\"\"\"\n",
    "    if not hasattr(model, \"feature_importances_\"):\n",
    "        return X.columns.tolist()\n",
    "    \n",
    "    feature_importances = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    total_features = len(importance_df)\n",
    "    additional_features = max(1, round(0.6 * total_features))\n",
    "    selected_features_count = min(len(X.columns), max(min_features, additional_features + min_features))\n",
    "    \n",
    "    selected_features = importance_df.head(selected_features_count)['Feature'].tolist()\n",
    "    return selected_features\n",
    "\n",
    "@ray.remote\n",
    "def train_and_evaluate_model(model_name, model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Trains and evaluates a model, selecting the best features if applicable.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    selected_features = select_top_features(model, X_train)\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = X_test[selected_features]\n",
    "    \n",
    "    model.fit(X_train_selected, y_train)\n",
    "    score = model.score(X_test_selected, y_test)\n",
    "    return model_name, score, selected_features\n",
    "\n",
    "def select_and_apply_model(X, y):\n",
    "    \"\"\"Selects the best model using Ray for parallel execution.\"\"\"\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    # Identify task type\n",
    "    task_type = \"regression\" if pd.api.types.is_numeric_dtype(y) else \"classification\"\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        \"regression\": {\n",
    "            \"LinearRegression\": LinearRegression(),\n",
    "            \"Lasso\": Lasso(alpha=0.1, random_state=42),\n",
    "            \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "            \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n",
    "            \"ExtraTreeRegressor\": ExtraTreeRegressor(),\n",
    "            \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            \"ExtraTreesRegressor\": ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
    "            \"AdaBoostRegressor\": AdaBoostRegressor(n_estimators=50, random_state=42),\n",
    "            \"XGBRegressor\": XGBRegressor(n_estimators=100, random_state=42)\n",
    "        },\n",
    "        \"classification\": {\n",
    "            \"LogisticRegression\": LogisticRegression(max_iter=200, random_state=42),\n",
    "            \"RidgeClassifier\": RidgeClassifier(),\n",
    "            \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "            \"ExtraTreeClassifier\": ExtraTreeClassifier(),\n",
    "            \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "            \"RadiusNeighborsClassifier\": RadiusNeighborsClassifier(),\n",
    "            \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            \"ExtraTreesClassifier\": ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train models in parallel\n",
    "    tasks = [train_and_evaluate_model.remote(name, model, X_train, X_test, y_train, y_test)\n",
    "             for name, model in models[task_type].items()]\n",
    "    results = ray.get(tasks)\n",
    "    \n",
    "    # Select the best model\n",
    "    best_model_name, best_model_score, best_features = max(results, key=lambda x: x[1])\n",
    "    best_model = models[task_type][best_model_name]\n",
    "    \n",
    "    print(f\"Best {task_type} model: {best_model_name} with score: {best_model_score:.4f}\")\n",
    "    print(f\"Selected Features: {best_features}\")\n",
    "    return best_model, best_model_name, best_model_score, best_features\n",
    "\n",
    "# Example usage\n",
    "y = df[\"DEP_DELAY\"]\n",
    "X = df.drop(columns=[\"DEP_DELAY\", \"FL_DATE\"])\n",
    "best_model, model_name, model_score, top_features = select_and_apply_model(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf68d32-edad-42f5-9626-5e95ddcc0b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
