{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d90c401-f0ad-45ba-80c0-e91a0bf7ed89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:38677' processes=8 threads=8, memory=14.90 GiB>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Create a Dask client with 8 workers and 2 threads per worker\n",
    "client = Client(n_workers=8, threads_per_worker=1, memory_limit='2GB')\n",
    "\n",
    "# Print out the details of the Dask cluster\n",
    "print(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbba54ce-ff4a-4cf6-8426-f36af98b5924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.stats import chi2\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, ExtraTreeRegressor, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier, AdaBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Suppress all warnings\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73476bc-c98c-4d59-b061-1cc57b35199a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BigQuery and Storage clients\n",
    "storage_client = storage.Client()\n",
    "\n",
    "print(\"Authentication successful!\")\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec42c57-baa2-456a-bfed-e9cd65abf0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import gcsfs\n",
    "\n",
    "\n",
    "\n",
    "# Load CSV into Dask DataFrame\n",
    "df = dd.read_csv(\n",
    "    \"gs://samplebucketvrs/Intelligence Analytics/og4/2019.csv\",\n",
    "    blocksize=\"50MB\",\n",
    "    assume_missing=True  # Converts int64 to float64 if missing values exist\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995ea98b-7112-4cd4-bef8-8eaa4d88f44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check Data\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9f2a8fe-d344-4714-924a-492179166b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Describe dataset (computes statistics on a sample)\n",
    "#print(df.describe().compute())\n",
    "\n",
    "# Drop 'id' column if it exists\n",
    "columns_to_drop = [col for col in df.columns if col.lower() == 'id']\n",
    "if columns_to_drop:\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Convert 'Date' column to datetime using Dask\n",
    "df['FL_DATE'] = df['FL_DATE'].map_partitions(pd.to_datetime, meta=('FL_DATE', 'datetime64[ns]'))\n",
    "\n",
    "# Verify changes\n",
    "dtypes = df.dtypes\n",
    "#print(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06b67c6-c8e9-41fb-97a1-741c2ddbd4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 20\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5d629-a3c0-466f-a803-d323ce485246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "def detect_outliers_dask(df, method=\"iqr\", threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers in a large Dask DataFrame using IQR or Z-score.\n",
    "    Returns a dictionary with column names and percentage of outliers.\n",
    "    \"\"\"\n",
    "    outlier_percentages = {}\n",
    "\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if method == \"iqr\":\n",
    "            Q1, Q3 = df[col].quantile([0.25, 0.75]).compute(split_out=4)  \n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "        elif method == \"zscore\":\n",
    "            mean, std = df[col].mean().compute(split_out=4), df[col].std().compute(split_out=4)\n",
    "            mask = abs((df[col] - mean) / std) > threshold\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'iqr' or 'zscore'.\")\n",
    "\n",
    "        outlier_count = mask.sum().compute()\n",
    "        total_rows = len(df)\n",
    "        outlier_percentages[col] = (outlier_count / total_rows) * 100\n",
    "    \n",
    "    return outlier_percentages\n",
    "\n",
    "def clean_or_winsorize_dask(df, outlier_percentages, threshold=5):\n",
    "    \"\"\"\n",
    "    Cleans or applies Winsorization based on outlier percentage.\n",
    "    \"\"\"\n",
    "    for col, percentage in outlier_percentages.items():\n",
    "        if percentage <= threshold:\n",
    "            print(f\"Removing {percentage:.2f}% outliers from column '{col}'.\")\n",
    "            Q1, Q3 = df[col].quantile([0.25, 0.75]).compute(split_out=4)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "        else:\n",
    "            print(f\"Applying Winsorization to column '{col}' with {percentage:.2f}% outliers.\")\n",
    "            df[col] = df[col].map_partitions(lambda x: winsorize(x, limits=(0.05, 0.05)), meta=(col, 'float64'))\n",
    "\n",
    "    return df\n",
    "\n",
    "# ✅ **Step 1: Detect Outliers in the Existing df**\n",
    "outlier_percentages = detect_outliers_dask(df, method=\"iqr\", threshold=1.5)\n",
    "\n",
    "# ✅ **Step 2: Clean Data**\n",
    "df_cleaned = clean_or_winsorize_dask(df, outlier_percentages, threshold=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937a813-a918-4e8d-a127-c0434bf2904c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dask_ml.preprocessing import LabelEncoder\n",
    "from dask import delayed\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\",\"string\"]).columns\n",
    "\n",
    "# ✅ Impute missing values for numerical columns using median\n",
    "for col in numerical_cols:\n",
    "    median_value = df[col].median_approximate().compute()  # Use approximate median for efficiency\n",
    "    df[col] = df[col].fillna(median_value)\n",
    "    print(f\"Imputed median for {col}: {median_value}\")\n",
    "\n",
    "# ✅ Encode categorical variables\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique().compute()  # Get number of unique values\n",
    "    \n",
    "    if unique_count <= 10:\n",
    "        # **One-Hot Encoding** for low-cardinality categorical columns\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        \n",
    "        # Apply One-Hot Encoding using map_partitions\n",
    "        df = df.map_partitions(\n",
    "            lambda d: d.join(\n",
    "                pd.DataFrame(\n",
    "                    encoder.fit_transform(d[[col]]),\n",
    "                    index=d.index,\n",
    "                    columns=[f\"{col}_{i}\" for i in range(unique_count)]\n",
    "                )\n",
    "            ), \n",
    "            meta=df\n",
    "        )\n",
    "        \n",
    "        df = df.drop(columns=[col])  # Drop original column after encoding\n",
    "        print(f\"Applied One-Hot Encoding to {col}\")\n",
    "\n",
    "    else:\n",
    "        # **Label Encoding** for high-cardinality categorical columns\n",
    "        encoder = LabelEncoder()\n",
    "        df[col] = df[col].map_partitions(lambda x: encoder.fit_transform(x), meta=(col, 'int64'))\n",
    "        print(f\"Applied Label Encoding to {col}\")\n",
    "\n",
    "# ✅ Check the transformed DataFrame\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6022b-d7a9-476d-b431-b9cb0d123a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f65f7-2ab6-4b05-98b3-4d04ac10e658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "import dask.delayed\n",
    "\n",
    "# ✅ Sample 10% of the data\n",
    "sampled_df = df.dropna(subset=[\"DEP_DELAY\"]).sample(frac=0.1, random_state=42)\n",
    "\n",
    "# ✅ Separate features (X) and target (y)\n",
    "target_col = \"DEP_DELAY\"\n",
    "y = sampled_df[target_col]\n",
    "X = sampled_df.drop(columns=[target_col, \"FL_DATE\"])\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Standardize numerical features with Dask's StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ✅ Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Compute Dask DataFrame to NumPy (for model compatibility)\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = [\n",
    "    arr.compute() for arr in [X_train, X_test, y_train, y_test]\n",
    "]\n",
    "\n",
    "# ✅ Ensure task type and model selection\n",
    "if y_train_np.nunique() <= 10:\n",
    "    model = ExtraTreesClassifier(random_state=42, n_jobs=-1, n_estimators=100)\n",
    "    task_type = \"Classification\"\n",
    "else:\n",
    "    model = ExtraTreesRegressor(random_state=42, n_jobs=-1, n_estimators=100)\n",
    "    task_type = \"Regression\"\n",
    "\n",
    "# ✅ Train the model\n",
    "model.fit(X_train_np, y_train_np)\n",
    "\n",
    "print(f\"✅ {task_type} Model Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942e9f0-6c04-487b-9579-3c4d4b93028e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d80871-e98b-40a7-8ef0-5c5513d1864e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
