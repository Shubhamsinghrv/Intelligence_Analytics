{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02cf17d-b916-4771-a0a5-24cf9e9651f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow running on: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force CPU usage\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress INFO and WARNING messages\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow running on:\", \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650f7ee1-b6c5-4c3c-8fd8-deff3582a073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7c806e-26f6-4202-b89b-645ecfcfd0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)  # Controls operations running in parallel\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)  # Controls parallelism within individual operations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from google.cloud import bigquery\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, PCA\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, DecisionTreeRegressor, GBTRegressor\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Suppress all warnings\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503b8d49-49a9-40bd-b681-001379e60bef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads used: 13\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"Threads used: {psutil.Process().num_threads()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde5d525-a4d4-4ae5-920a-e85961d240ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BigQuery and Storage clients\n",
    "storage_client = storage.Client()\n",
    "client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "print(\"Authentication successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb7e820-f5d3-41bc-b7a5-06663bbc98b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 07:19:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 60242)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigQueryLoader\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/spark-bigquery-latest_2.12.jar\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2421500d-2807-4bd4-806d-a89db5d44cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext._jsc.hadoopConfiguration().get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15d0471-fcb7-447c-9660-6e1f39e2718b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file into big-data-engin.Sample1.airplane2 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 07:43:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: long (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEP_TIME: long (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- WHEELS_OFF: long (nullable = true)\n",
      " |-- WHEELS_ON: long (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- ARR_TIME: long (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- CARRIER_DELAY: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- SECURITY_DELAY: double (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: double (nullable = true)\n",
      " |-- string_field_20: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_UNIQUE_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>...</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>string_field_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-08</td>\n",
       "      <td>AS</td>\n",
       "      <td>64</td>\n",
       "      <td>PSG</td>\n",
       "      <td>WRG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-24</td>\n",
       "      <td>AS</td>\n",
       "      <td>64</td>\n",
       "      <td>PSG</td>\n",
       "      <td>WRG</td>\n",
       "      <td>1505.00</td>\n",
       "      <td>-15.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1507.00</td>\n",
       "      <td>1519.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1522.00</td>\n",
       "      <td>-23.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-16</td>\n",
       "      <td>AS</td>\n",
       "      <td>64</td>\n",
       "      <td>PSG</td>\n",
       "      <td>WRG</td>\n",
       "      <td>1458.00</td>\n",
       "      <td>-27.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1507.00</td>\n",
       "      <td>1519.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1522.00</td>\n",
       "      <td>-23.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>1028.00</td>\n",
       "      <td>-17.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1032.00</td>\n",
       "      <td>1040.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1044.00</td>\n",
       "      <td>-21.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>AS</td>\n",
       "      <td>64</td>\n",
       "      <td>PSG</td>\n",
       "      <td>WRG</td>\n",
       "      <td>1505.00</td>\n",
       "      <td>-24.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1516.00</td>\n",
       "      <td>1529.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1532.00</td>\n",
       "      <td>-21.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FL_DATE OP_UNIQUE_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  DEP_TIME  \\\n",
       "0  2019-11-08                AS                 64    PSG  WRG       NaN   \n",
       "1  2019-01-24                AS                 64    PSG  WRG   1505.00   \n",
       "2  2019-03-16                AS                 64    PSG  WRG   1458.00   \n",
       "3  2019-01-30                AS                 65    WRG  PSG   1028.00   \n",
       "4  2019-01-03                AS                 64    PSG  WRG   1505.00   \n",
       "\n",
       "   DEP_DELAY  TAXI_OUT  WHEELS_OFF  WHEELS_ON  ...  ARR_TIME  ARR_DELAY  \\\n",
       "0        NaN       NaN         NaN        NaN  ...       NaN        NaN   \n",
       "1     -15.00      2.00     1507.00    1519.00  ...   1522.00     -23.00   \n",
       "2     -27.00      9.00     1507.00    1519.00  ...   1522.00     -23.00   \n",
       "3     -17.00      4.00     1032.00    1040.00  ...   1044.00     -21.00   \n",
       "4     -24.00     11.00     1516.00    1529.00  ...   1532.00     -21.00   \n",
       "\n",
       "   AIR_TIME  DISTANCE  CARRIER_DELAY  WEATHER_DELAY  NAS_DELAY  \\\n",
       "0       NaN     31.00            NaN            NaN        NaN   \n",
       "1     12.00     31.00            NaN            NaN        NaN   \n",
       "2     12.00     31.00            NaN            NaN        NaN   \n",
       "3      8.00     31.00            NaN            NaN        NaN   \n",
       "4     13.00     31.00            NaN            NaN        NaN   \n",
       "\n",
       "   SECURITY_DELAY  LATE_AIRCRAFT_DELAY  string_field_20  \n",
       "0             NaN                  NaN             None  \n",
       "1             NaN                  NaN             None  \n",
       "2             NaN                  NaN             None  \n",
       "3             NaN                  NaN             None  \n",
       "4             NaN                  NaN             None  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define GCP details\n",
    "project_id = \"big-data-engin\"  # Replace with your actual GCP project ID\n",
    "dataset_id = \"Sample1\"\n",
    "table_id = \"airplane2\"\n",
    "\n",
    "# Full table ID for BigQuery\n",
    "full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Define GCS URI (Google Cloud Storage path)\n",
    "gcs_uri = \"gs://samplebucketvrs/Intelligence Analytics/og4/2019.csv\"  # Replace with actual GCS path\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Configure BigQuery job\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,  # Skip the header row\n",
    "    autodetect=True,  # Enable schema autodetection\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND  # Append data if table exists\n",
    ")\n",
    "\n",
    "# Load data from GCS into BigQuery\n",
    "load_job = client.load_table_from_uri(gcs_uri, full_table_id, job_config=job_config)\n",
    "\n",
    "# Wait for job to complete before proceeding\n",
    "load_job.result()\n",
    "print(f\"Loaded CSV file into {full_table_id} successfully.\")\n",
    "\n",
    "# Initialize Spark session (if running in a PySpark environment)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigQueryLoad\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from BigQuery into a PySpark DataFrame\n",
    "df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", full_table_id) \\\n",
    "    .load()\n",
    "\n",
    "# Show schema and first few rows\n",
    "df.printSchema()\n",
    "#df.show(5)\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522f985b-9df3-4422-9e8f-9814793145e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()\n",
    "columns_to_drop = [col for col in df.columns if col.lower() == 'id']\n",
    "\n",
    "if columns_to_drop:\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "df = df.withColumn('FL_DATE', df['FL_DATE'].cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd26ca2-97ad-48bf-8dff-932bd28d2c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|             Column|       Type|\n",
      "+-------------------+-----------+\n",
      "|            FL_DATE|   Datetime|\n",
      "|  OP_UNIQUE_CARRIER|Categorical|\n",
      "|  OP_CARRIER_FL_NUM|  Numerical|\n",
      "|             ORIGIN|Categorical|\n",
      "|               DEST|Categorical|\n",
      "|           DEP_TIME|  Numerical|\n",
      "|          DEP_DELAY|  Numerical|\n",
      "|           TAXI_OUT|  Numerical|\n",
      "|         WHEELS_OFF|  Numerical|\n",
      "|          WHEELS_ON|  Numerical|\n",
      "|            TAXI_IN|  Numerical|\n",
      "|           ARR_TIME|  Numerical|\n",
      "|          ARR_DELAY|  Numerical|\n",
      "|           AIR_TIME|  Numerical|\n",
      "|           DISTANCE|  Numerical|\n",
      "|      CARRIER_DELAY|  Numerical|\n",
      "|      WEATHER_DELAY|  Numerical|\n",
      "|          NAS_DELAY|  Numerical|\n",
      "|     SECURITY_DELAY|  Numerical|\n",
      "|LATE_AIRCRAFT_DELAY|  Numerical|\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType, NumericType, StringType, DateType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def identify_column_type(df):\n",
    "    column_types = []\n",
    "\n",
    "    for field in df.schema.fields:  # Proper way to get schema fields in PySpark\n",
    "        col_name = field.name\n",
    "        dtype = field.dataType\n",
    "\n",
    "        if isinstance(dtype, BooleanType):\n",
    "            col_type = \"Boolean\"\n",
    "        elif isinstance(dtype, NumericType):\n",
    "            col_type = \"Numerical\"\n",
    "        elif isinstance(dtype, StringType):\n",
    "            col_type = \"Categorical\"\n",
    "        elif isinstance(dtype, (DateType, TimestampType)):\n",
    "            col_type = \"Datetime\"\n",
    "        else:\n",
    "            col_type = \"Unknown\"\n",
    "        \n",
    "        column_types.append((col_name, col_type))\n",
    "\n",
    "    # Convert list of tuples into a Spark DataFrame\n",
    "    schema = [\"Column\", \"Type\"]\n",
    "    column_types_df = spark.createDataFrame(column_types, schema=schema)\n",
    "    \n",
    "    return column_types_df\n",
    "\n",
    "# Identify column types\n",
    "column_types_df = identify_column_type(df)\n",
    "column_types_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f88e1b1-8db9-494d-9433-84ddc22dcb15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 07:44:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'DEP_DELAY' with 13.41% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4.30% outliers from column 'TAXI_OUT'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'TAXI_IN' with 6.19% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'ARR_DELAY' with 9.14% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'AIR_TIME' with 5.31% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'DISTANCE' with 5.91% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2.16% outliers from column 'CARRIER_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1.11% outliers from column 'WEATHER_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1.46% outliers from column 'NAS_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.06% outliers from column 'SECURITY_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 119:===================================================>   (32 + 2) / 34]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1.83% outliers from column 'LATE_AIRCRAFT_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, when, count, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"OutlierDetection\").getOrCreate()\n",
    "\n",
    "def detect_outliers_large(df, method=\"iqr\", threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers in a PySpark DataFrame using IQR or Z-score.\n",
    "    Returns a dictionary of outlier counts per column.\n",
    "    \"\"\"\n",
    "    outliers = {}\n",
    "\n",
    "    if method == \"iqr\":\n",
    "        for col_name in df.columns:\n",
    "            if df.select(col_name).schema.fields[0].dataType.simpleString() in [\"int\", \"double\", \"float\"]:\n",
    "                # Compute Q1, Q3, and IQR using Spark SQL functions\n",
    "                Q1 = df.approxQuantile(col_name, [0.25], 0.01)[0]\n",
    "                Q3 = df.approxQuantile(col_name, [0.75], 0.01)[0]\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "                # Count outliers\n",
    "                outlier_count = df.filter((col(col_name) < lower_bound) | (col(col_name) > upper_bound)).count()\n",
    "                outliers[col_name] = outlier_count\n",
    "\n",
    "    elif method == \"zscore\":\n",
    "        for col_name in df.columns:\n",
    "            if df.select(col_name).schema.fields[0].dataType.simpleString() in [\"int\", \"double\", \"float\"]:\n",
    "                stats = df.select(\n",
    "                    mean(col(col_name)).alias(\"mean\"),\n",
    "                    stddev(col(col_name)).alias(\"stddev\")\n",
    "                ).collect()[0]\n",
    "\n",
    "                mean_val, std_val = stats[\"mean\"], stats[\"stddev\"]\n",
    "                \n",
    "                # Count outliers\n",
    "                outlier_count = df.filter(((col(col_name) - mean_val) / std_val).abs() > threshold).count()\n",
    "                outliers[col_name] = outlier_count\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'iqr' or 'zscore'.\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "def calculate_outlier_percentage(outliers_dict, total_rows):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of outliers for each column.\n",
    "    \"\"\"\n",
    "    outlier_percentage = {col: (count / total_rows) * 100 for col, count in outliers_dict.items()}\n",
    "    return outlier_percentage\n",
    "\n",
    "def clean_or_winsorize(df, outliers_dict, threshold=5):\n",
    "    \"\"\"\n",
    "    Clean or apply Winsorization based on outlier percentage.\n",
    "\n",
    "    Parameters:\n",
    "        df (Spark DataFrame): Input DataFrame.\n",
    "        outliers_dict (dict): Dictionary with outlier counts.\n",
    "        threshold (int): Percentage threshold to decide action.\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    outlier_percentage = calculate_outlier_percentage(outliers_dict, total_rows)\n",
    "\n",
    "    for col_name, percentage in outlier_percentage.items():\n",
    "        if percentage <= threshold:\n",
    "            # Remove outliers\n",
    "            Q1 = df.approxQuantile(col_name, [0.25], 0.01)[0]\n",
    "            Q3 = df.approxQuantile(col_name, [0.75], 0.01)[0]\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            df = df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))\n",
    "            print(f\"Removed {percentage:.2f}% outliers from column '{col_name}'.\")\n",
    "        else:\n",
    "            # Apply Winsorization using percentile approximation\n",
    "            lower_bound = df.approxQuantile(col_name, [0.05], 0.01)[0]\n",
    "            upper_bound = df.approxQuantile(col_name, [0.95], 0.01)[0]\n",
    "\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                when(col(col_name) < lower_bound, lower_bound)\n",
    "                .when(col(col_name) > upper_bound, upper_bound)\n",
    "                .otherwise(col(col_name))\n",
    "            )\n",
    "            print(f\"Applied Winsorization to column '{col_name}' with {percentage:.2f}% outliers.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example Usage:\n",
    "# Load dataset from BigQuery or CSV\n",
    "# df = spark.read.format(\"bigquery\").option(\"table\", \"your_project.your_dataset.your_table\").load()\n",
    "# df = spark.read.csv(\"your_large_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "total_rows = df.count()\n",
    "outliers_dict = detect_outliers_large(df, method=\"iqr\", threshold=1.5)\n",
    "\n",
    "# Process data based on outlier percentage\n",
    "df_processed = clean_or_winsorize(df, outliers_dict, threshold=5)\n",
    "\n",
    "# Save the processed dataset\n",
    "# df_processed.write.csv(\"cleaned_dataset.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd69c44a-92c9-4dba-87de-4478bea6ee3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\n# Initialize Spark\\nspark = SparkSession.builder.appName(\"PandasToSpark\").getOrCreate()\\n# Now pass it to the function\\ncolumn_types_df = identify_column_type(df)\\ncolumn_types_df.show()'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"PandasToSpark\").getOrCreate()\n",
    "# Now pass it to the function\n",
    "column_types_df = identify_column_type(df)\n",
    "column_types_df.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333ba4e-2e55-4101-babb-e8f60d615fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 07:50:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping column 'string_field_20' (only 0 unique value).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2152.554s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2152.555s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"block-manager-storage-async-thread-pool-448\"\n",
      "[2152.555s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2152.555s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"block-manager-storage-async-thread-pool-449\"\n",
      "[2152.555s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2152.555s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"block-manager-storage-async-thread-pool-450\"\n",
      "[2152.556s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2152.556s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"block-manager-ask-thread-pool-329\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"block-manager-storage-async-thread-pool-443\" java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1005)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "java.lang.IllegalStateException: problem in scala.concurrent internal callback\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.reportFailure(Future.scala:877)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:223)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.sendFailure(NettyRpcCallContext.scala:36)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:108)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed\n",
      "\tat scala.Predef$.require(Predef.scala:268)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:76)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\t... 34 more\n",
      "25/02/05 07:55:39 ERROR Inbox: An error happened while processing message in the inbox for BlockManagerEndpoint1\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1343)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Kept.onComplete(Promise.scala:372)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Kept.onComplete$(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.onComplete(Promise.scala:379)\n",
      "\tat scala.concurrent.impl.Promise.transform(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.transform$(Promise.scala:31)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.transform(Promise.scala:379)\n",
      "\tat scala.concurrent.Future.map(Future.scala:292)\n",
      "\tat scala.concurrent.Future.map$(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.map(Promise.scala:379)\n",
      "\tat scala.concurrent.Future$.apply(Future.scala:659)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.org$apache$spark$storage$BlockManagerStorageEndpoint$$doAsync(BlockManagerStorageEndpoint.scala:99)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Exception in thread \"dispatcher-BlockManagerEndpoint1\" java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1343)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Kept.onComplete(Promise.scala:372)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Kept.onComplete$(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.onComplete(Promise.scala:379)\n",
      "\tat scala.concurrent.impl.Promise.transform(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.transform$(Promise.scala:31)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.transform(Promise.scala:379)\n",
      "\tat scala.concurrent.Future.map(Future.scala:292)\n",
      "\tat scala.concurrent.Future.map$(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.map(Promise.scala:379)\n",
      "\tat scala.concurrent.Future$.apply(Future.scala:659)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.org$apache$spark$storage$BlockManagerStorageEndpoint$$doAsync(BlockManagerStorageEndpoint.scala:99)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2152.999s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2152.999s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"Gax-9246\"\n",
      "[2153.002s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2153.002s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"Gax-9247\"\n",
      "[2153.004s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2153.004s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"Gax-9248\"\n",
      "[2153.006s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2153.006s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"Gax-9249\"\n",
      "[2153.110s][warning][os,thread] Failed to start thread \"Unknown thread\" - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[2153.111s][warning][os,thread] Failed to start the native thread for java.lang.Thread \"Thread-10407\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 07:55:39 ERROR Executor: Exception in task 2.0 in stage 175.0 (TID 1571)\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1583)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:346)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:632)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.start(Watchdog.java:94)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.create(Watchdog.java:82)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.InstantiatingWatchdogProvider.getWatchdog(InstantiatingWatchdogProvider.java:111)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.ClientContext.create(ClientContext.java:223)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:98)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:130)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.createBigQueryReadClient(BigQueryClientFactory.java:168)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.getBigQueryReadClient(BigQueryClientFactory.java:74)\n",
      "\tat com.google.cloud.bigquery.connector.common.ReadRowsHelper.readRows(ReadRowsHelper.java:106)\n",
      "\tat com.google.cloud.spark.bigquery.direct.PreScala213BigQueryRDD.compute(PreScala213BigQueryRDD.java:102)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/02/05 07:55:39 ERROR Executor: Exception in task 0.0 in stage 175.0 (TID 1569)\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1583)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:346)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:632)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.start(Watchdog.java:94)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.create(Watchdog.java:82)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.InstantiatingWatchdogProvider.getWatchdog(InstantiatingWatchdogProvider.java:111)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.ClientContext.create(ClientContext.java:223)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:98)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:130)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.createBigQueryReadClient(BigQueryClientFactory.java:168)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.getBigQueryReadClient(BigQueryClientFactory.java:74)\n",
      "\tat com.google.cloud.bigquery.connector.common.ReadRowsHelper.readRows(ReadRowsHelper.java:106)\n",
      "\tat com.google.cloud.spark.bigquery.direct.PreScala213BigQueryRDD.compute(PreScala213BigQueryRDD.java:102)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/02/05 07:55:39 ERROR Executor: Exception in task 1.0 in stage 175.0 (TID 1570)\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1583)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:346)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:632)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.start(Watchdog.java:94)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.create(Watchdog.java:82)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.InstantiatingWatchdogProvider.getWatchdog(InstantiatingWatchdogProvider.java:111)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.ClientContext.create(ClientContext.java:223)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:98)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:130)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.createBigQueryReadClient(BigQueryClientFactory.java:168)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.getBigQueryReadClient(BigQueryClientFactory.java:74)\n",
      "\tat com.google.cloud.bigquery.connector.common.ReadRowsHelper.readRows(ReadRowsHelper.java:106)\n",
      "\tat com.google.cloud.spark.bigquery.direct.PreScala213BigQueryRDD.compute(PreScala213BigQueryRDD.java:102)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/02/05 07:55:39 ERROR Executor: Exception in task 3.0 in stage 175.0 (TID 1572)\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1583)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:346)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:632)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.start(Watchdog.java:94)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.create(Watchdog.java:82)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.InstantiatingWatchdogProvider.getWatchdog(InstantiatingWatchdogProvider.java:111)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.ClientContext.create(ClientContext.java:223)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:98)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:130)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.createBigQueryReadClient(BigQueryClientFactory.java:168)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.getBigQueryReadClient(BigQueryClientFactory.java:74)\n",
      "\tat com.google.cloud.bigquery.connector.common.ReadRowsHelper.readRows(ReadRowsHelper.java:106)\n",
      "\tat com.google.cloud.spark.bigquery.direct.PreScala213BigQueryRDD.compute(PreScala213BigQueryRDD.java:102)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/02/05 07:55:39 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 175.0 (TID 1571),5,main]\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1583)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:346)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:632)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.start(Watchdog.java:94)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.create(Watchdog.java:82)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.InstantiatingWatchdogProvider.getWatchdog(InstantiatingWatchdogProvider.java:111)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.ClientContext.create(ClientContext.java:223)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:98)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:130)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.createBigQueryReadClient(BigQueryClientFactory.java:168)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.getBigQueryReadClient(BigQueryClientFactory.java:74)\n",
      "\tat com.google.cloud.bigquery.connector.common.ReadRowsHelper.readRows(ReadRowsHelper.java:106)\n",
      "\tat com.google.cloud.spark.bigquery.direct.PreScala213BigQueryRDD.compute(PreScala213BigQueryRDD.java:102)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/02/05 07:55:39 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 175.0 (TID 1570),5,main]\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1583)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:346)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:632)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.start(Watchdog.java:94)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.create(Watchdog.java:82)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.InstantiatingWatchdogProvider.getWatchdog(InstantiatingWatchdogProvider.java:111)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.ClientContext.create(ClientContext.java:223)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:98)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:130)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.createBigQueryReadClient(BigQueryClientFactory.java:168)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.getBigQueryReadClient(BigQueryClientFactory.java:74)\n",
      "\tat com.google.cloud.bigquery.connector.common.ReadRowsHelper.readRows(ReadRowsHelper.java:106)\n",
      "\tat com.google.cloud.spark.bigquery.direct.PreScala213BigQueryRDD.compute(PreScala213BigQueryRDD.java:102)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/02/05 07:55:39 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 175.0 (TID 1569),5,main]\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1583)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:346)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:632)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.start(Watchdog.java:94)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.create(Watchdog.java:82)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.InstantiatingWatchdogProvider.getWatchdog(InstantiatingWatchdogProvider.java:111)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.ClientContext.create(ClientContext.java:223)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:98)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:130)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.createBigQueryReadClient(BigQueryClientFactory.java:168)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.getBigQueryReadClient(BigQueryClientFactory.java:74)\n",
      "\tat com.google.cloud.bigquery.connector.common.ReadRowsHelper.readRows(ReadRowsHelper.java:106)\n",
      "\tat com.google.cloud.spark.bigquery.direct.PreScala213BigQueryRDD.compute(PreScala213BigQueryRDD.java:102)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/02/05 07:55:39 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 175.0 (TID 1572),5,main]\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.ensurePrestart(ThreadPoolExecutor.java:1583)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:346)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleAtFixedRate(ScheduledThreadPoolExecutor.java:632)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.start(Watchdog.java:94)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.Watchdog.create(Watchdog.java:82)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.InstantiatingWatchdogProvider.getWatchdog(InstantiatingWatchdogProvider.java:111)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.rpc.ClientContext.create(ClientContext.java:223)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.stub.EnhancedBigQueryReadStub.create(EnhancedBigQueryReadStub.java:98)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.<init>(BigQueryReadClient.java:130)\n",
      "\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.storage.v1.BigQueryReadClient.create(BigQueryReadClient.java:110)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.createBigQueryReadClient(BigQueryClientFactory.java:168)\n",
      "\tat com.google.cloud.bigquery.connector.common.BigQueryClientFactory.getBigQueryReadClient(BigQueryClientFactory.java:74)\n",
      "\tat com.google.cloud.bigquery.connector.common.ReadRowsHelper.readRows(ReadRowsHelper.java:106)\n",
      "\tat com.google.cloud.spark.bigquery.direct.PreScala213BigQueryRDD.compute(PreScala213BigQueryRDD.java:102)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/envs/tensorflow/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o951.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Create and apply the transformation pipeline\u001b[39;00m\n\u001b[1;32m     58\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39mindexers \u001b[38;5;241m+\u001b[39m encoders \u001b[38;5;241m+\u001b[39m imputers)\n\u001b[0;32m---> 59\u001b[0m df_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Show transformed dataset\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#df_transformed.show()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o951.fit"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Compute unique counts for each column\n",
    "unique_counts_df = (\n",
    "    df.select([countDistinct(col(c)).alias(c) for c in df.columns])\n",
    "    .toPandas()\n",
    "    .melt(var_name=\"Column\", value_name=\"Unique_Values\")\n",
    ")\n",
    "\n",
    "# Convert Pandas DataFrame to PySpark DataFrame with explicit schema\n",
    "unique_counts_spark_df = spark.createDataFrame(\n",
    "    unique_counts_df,\n",
    "    schema=StructType([\n",
    "        StructField(\"Column\", StringType(), True),\n",
    "        StructField(\"Unique_Values\", IntegerType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Ensure 'column_types_df' exists and join with unique counts\n",
    "if \"column_types_df\" in locals():\n",
    "    column_info = column_types_df.join(unique_counts_spark_df, on=\"Column\")\n",
    "else:\n",
    "    raise ValueError(\"column_types_df is not defined. Ensure you run 'identify_column_type(df)' first.\")\n",
    "\n",
    "# Initialize transformation lists\n",
    "indexers, encoders, imputers = [], [], []\n",
    "\n",
    "# Process each column's metadata\n",
    "for row in column_info.rdd.collect():  \n",
    "    col_name = row[\"Column\"]\n",
    "    col_type = row[\"Type\"]\n",
    "    unique_values = row[\"Unique_Values\"]\n",
    "\n",
    "    if col_type in [\"Categorical\", \"Boolean\"]:\n",
    "        if unique_values <= 1:\n",
    "            print(f\"Skipping column '{col_name}' (only {unique_values} unique value).\")\n",
    "            continue\n",
    "        \n",
    "        indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
    "        indexers.append(indexer)\n",
    "        \n",
    "        if unique_values <= 10:\n",
    "            encoder = OneHotEncoder(inputCol=f\"{col_name}_index\", outputCol=f\"{col_name}_encoded\")\n",
    "            encoders.append(encoder)\n",
    "\n",
    "    elif col_type == \"Numerical\":\n",
    "        imputer = Imputer(inputCols=[col_name], outputCols=[col_name + \"_imputed\"], strategy=\"median\")\n",
    "        imputers.append(imputer)\n",
    "\n",
    "# Create and apply the transformation pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + imputers)\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Show transformed dataset\n",
    "#df_transformed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d11c6d-0299-4860-bcd8-376a3a3493b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_transformed.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23bc0a-0491-4dcb-b7c7-baa88f9a14c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Modeling\").getOrCreate()\n",
    "\n",
    "\n",
    "# Sample 90% of the data\n",
    "sampled_df = df_transformed.sample(fraction=0.9, seed=42)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "y = \"DEP_DELAY\"\n",
    "X = sampled_df.drop(*[\"ARR_DELAY\", \"FL_DATE\"])\n",
    "\n",
    "# Convert numerical columns to features\n",
    "numeric_cols = [c for c in X.columns if dict(X.dtypes)[c] in ['int', 'double']]\n",
    "\n",
    "# Create a feature vector by combining all columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "X_assembled = assembler.transform(X)\n",
    "\n",
    "# Scale only numerical columns\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(X_assembled)\n",
    "X_scaled = scaler_model.transform(X_assembled)\n",
    "\n",
    "# Train/Test Split (80/20)\n",
    "train_df, test_df = X_scaled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define the model based on the target variable type (Regression or Classification)\n",
    "if dict(sampled_df.dtypes)[y] in ['int', 'double']:\n",
    "    # Regression Task\n",
    "    model = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=y, numTrees=100, seed=42)\n",
    "    evaluator = RegressionEvaluator(labelCol=y)\n",
    "    task_type = \"Regression\"\n",
    "else:\n",
    "    # Classification Task\n",
    "    model = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=y, numTrees=100, seed=42)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=y)\n",
    "    task_type = \"Classification\"\n",
    "\n",
    "# Train the model\n",
    "model_fitted = model.fit(train_df)\n",
    "\n",
    "# Get Feature Importances (Not directly available in PySpark, so we get model's feature importances)\n",
    "feature_importances = model_fitted.featureImportances\n",
    "\n",
    "# Convert the importances into a DataFrame for easy reading\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': numeric_cols,\n",
    "    'Importance': feature_importances.toArray()\n",
    "})\n",
    "\n",
    "# Show top features\n",
    "importance_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57377f48-532c-44f9-9d6a-622629d7c966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc7fe3-ea4a-4686-a120-170c99a76c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
