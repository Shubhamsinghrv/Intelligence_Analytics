{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02cf17d-b916-4771-a0a5-24cf9e9651f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow running on: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force CPU usage\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress INFO and WARNING messages\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow running on:\", \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650f7ee1-b6c5-4c3c-8fd8-deff3582a073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7c806e-26f6-4202-b89b-645ecfcfd0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)  # Controls operations running in parallel\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)  # Controls parallelism within individual operations\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from google.cloud import bigquery\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, PCA\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, DecisionTreeRegressor, GBTRegressor\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Suppress all warnings\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde5d525-a4d4-4ae5-920a-e85961d240ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "# Initialize BigQuery and Storage clients\n",
    "storage_client = storage.Client()\n",
    "client = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "print(\"Authentication successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb7e820-f5d3-41bc-b7a5-06663bbc98b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 09:33:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigQueryLoader\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/spark-bigquery-latest_2.12.jar\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2421500d-2807-4bd4-806d-a89db5d44cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext._jsc.hadoopConfiguration().get(\"spark.jars\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a15d0471-fcb7-447c-9660-6e1f39e2718b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file into big-data-engin.Sample1.airplane2 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 09:48:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: long (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEP_TIME: long (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- WHEELS_OFF: long (nullable = true)\n",
      " |-- WHEELS_ON: long (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- ARR_TIME: long (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- CARRIER_DELAY: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- SECURITY_DELAY: double (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: double (nullable = true)\n",
      " |-- string_field_20: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_UNIQUE_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>...</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>string_field_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>1028.00</td>\n",
       "      <td>-17.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1033.00</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1045.00</td>\n",
       "      <td>-25.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>AS</td>\n",
       "      <td>64</td>\n",
       "      <td>PSG</td>\n",
       "      <td>WRG</td>\n",
       "      <td>1504.00</td>\n",
       "      <td>-16.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1508.00</td>\n",
       "      <td>1517.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1521.00</td>\n",
       "      <td>-24.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-06</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>1008.00</td>\n",
       "      <td>-22.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1016.00</td>\n",
       "      <td>1025.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1029.00</td>\n",
       "      <td>-21.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>1031.00</td>\n",
       "      <td>-14.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1035.00</td>\n",
       "      <td>1045.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1047.00</td>\n",
       "      <td>-18.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FL_DATE OP_UNIQUE_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  DEP_TIME  \\\n",
       "0  2019-03-05                AS                 65    WRG  PSG       NaN   \n",
       "1  2019-05-05                AS                 65    WRG  PSG   1028.00   \n",
       "2  2019-01-14                AS                 64    PSG  WRG   1504.00   \n",
       "3  2019-07-06                AS                 65    WRG  PSG   1008.00   \n",
       "4  2019-05-13                AS                 65    WRG  PSG   1031.00   \n",
       "\n",
       "   DEP_DELAY  TAXI_OUT  WHEELS_OFF  WHEELS_ON  ...  ARR_TIME  ARR_DELAY  \\\n",
       "0        NaN       NaN         NaN        NaN  ...       NaN        NaN   \n",
       "1     -17.00      5.00     1033.00    1042.00  ...   1045.00     -25.00   \n",
       "2     -16.00      4.00     1508.00    1517.00  ...   1521.00     -24.00   \n",
       "3     -22.00      8.00     1016.00    1025.00  ...   1029.00     -21.00   \n",
       "4     -14.00      4.00     1035.00    1045.00  ...   1047.00     -18.00   \n",
       "\n",
       "   AIR_TIME  DISTANCE  CARRIER_DELAY  WEATHER_DELAY  NAS_DELAY  \\\n",
       "0       NaN     31.00            NaN            NaN        NaN   \n",
       "1      9.00     31.00            NaN            NaN        NaN   \n",
       "2      9.00     31.00            NaN            NaN        NaN   \n",
       "3      9.00     31.00            NaN            NaN        NaN   \n",
       "4     10.00     31.00            NaN            NaN        NaN   \n",
       "\n",
       "   SECURITY_DELAY  LATE_AIRCRAFT_DELAY  string_field_20  \n",
       "0             NaN                  NaN             None  \n",
       "1             NaN                  NaN             None  \n",
       "2             NaN                  NaN             None  \n",
       "3             NaN                  NaN             None  \n",
       "4             NaN                  NaN             None  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define GCP details\n",
    "project_id = \"big-data-engin\"  # Replace with your actual GCP project ID\n",
    "dataset_id = \"Sample1\"\n",
    "table_id = \"airplane2\"\n",
    "\n",
    "# Full table ID for BigQuery\n",
    "full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Define GCS URI (Google Cloud Storage path)\n",
    "gcs_uri = \"gs://samplebucketvrs/Intelligence Analytics/og4/2019.csv\"  # Replace with actual GCS path\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Configure BigQuery job\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,  # Skip the header row\n",
    "    autodetect=True,  # Enable schema autodetection\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND  # Append data if table exists\n",
    ")\n",
    "\n",
    "# Load data from GCS into BigQuery\n",
    "load_job = client.load_table_from_uri(gcs_uri, full_table_id, job_config=job_config)\n",
    "\n",
    "# Wait for job to complete before proceeding\n",
    "load_job.result()\n",
    "print(f\"Loaded CSV file into {full_table_id} successfully.\")\n",
    "\n",
    "# Initialize Spark session (if running in a PySpark environment)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigQueryLoad\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from BigQuery into a PySpark DataFrame\n",
    "df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", full_table_id) \\\n",
    "    .load()\n",
    "\n",
    "# Show schema and first few rows\n",
    "df.printSchema()\n",
    "#df.show(5)\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "522f985b-9df3-4422-9e8f-9814793145e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()\n",
    "columns_to_drop = [col for col in df.columns if col.lower() == 'id']\n",
    "\n",
    "if columns_to_drop:\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "df = df.withColumn('FL_DATE', df['FL_DATE'].cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd26ca2-97ad-48bf-8dff-932bd28d2c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|             Column|       Type|\n",
      "+-------------------+-----------+\n",
      "|            FL_DATE|   Datetime|\n",
      "|  OP_UNIQUE_CARRIER|Categorical|\n",
      "|  OP_CARRIER_FL_NUM|  Numerical|\n",
      "|             ORIGIN|Categorical|\n",
      "|               DEST|Categorical|\n",
      "|           DEP_TIME|  Numerical|\n",
      "|          DEP_DELAY|  Numerical|\n",
      "|           TAXI_OUT|  Numerical|\n",
      "|         WHEELS_OFF|  Numerical|\n",
      "|          WHEELS_ON|  Numerical|\n",
      "|            TAXI_IN|  Numerical|\n",
      "|           ARR_TIME|  Numerical|\n",
      "|          ARR_DELAY|  Numerical|\n",
      "|           AIR_TIME|  Numerical|\n",
      "|           DISTANCE|  Numerical|\n",
      "|      CARRIER_DELAY|  Numerical|\n",
      "|      WEATHER_DELAY|  Numerical|\n",
      "|          NAS_DELAY|  Numerical|\n",
      "|     SECURITY_DELAY|  Numerical|\n",
      "|LATE_AIRCRAFT_DELAY|  Numerical|\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import BooleanType, NumericType, StringType, DateType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def identify_column_type(df):\n",
    "    column_types = []\n",
    "\n",
    "    for field in df.schema.fields:  # Proper way to get schema fields in PySpark\n",
    "        col_name = field.name\n",
    "        dtype = field.dataType\n",
    "\n",
    "        if isinstance(dtype, BooleanType):\n",
    "            col_type = \"Boolean\"\n",
    "        elif isinstance(dtype, NumericType):\n",
    "            col_type = \"Numerical\"\n",
    "        elif isinstance(dtype, StringType):\n",
    "            col_type = \"Categorical\"\n",
    "        elif isinstance(dtype, (DateType, TimestampType)):\n",
    "            col_type = \"Datetime\"\n",
    "        else:\n",
    "            col_type = \"Unknown\"\n",
    "        \n",
    "        column_types.append((col_name, col_type))\n",
    "\n",
    "    # Convert list of tuples into a Spark DataFrame\n",
    "    schema = [\"Column\", \"Type\"]\n",
    "    column_types_df = spark.createDataFrame(column_types, schema=schema)\n",
    "    \n",
    "    return column_types_df\n",
    "\n",
    "# Identify column types\n",
    "column_types_df = identify_column_type(df)\n",
    "column_types_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f88e1b1-8db9-494d-9433-84ddc22dcb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 09:34:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'DEP_DELAY' with 13.41% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4.30% outliers from column 'TAXI_OUT'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'TAXI_IN' with 6.19% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'ARR_DELAY' with 9.14% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'AIR_TIME' with 5.31% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied Winsorization to column 'DISTANCE' with 5.89% outliers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2.16% outliers from column 'CARRIER_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1.11% outliers from column 'WEATHER_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1.46% outliers from column 'NAS_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0.06% outliers from column 'SECURITY_DELAY'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1.83% outliers from column 'LATE_AIRCRAFT_DELAY'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, when, count, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"OutlierDetection\").getOrCreate()\n",
    "\n",
    "def detect_outliers_large(df, method=\"iqr\", threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers in a PySpark DataFrame using IQR or Z-score.\n",
    "    Returns a dictionary of outlier counts per column.\n",
    "    \"\"\"\n",
    "    outliers = {}\n",
    "\n",
    "    if method == \"iqr\":\n",
    "        for col_name in df.columns:\n",
    "            if df.select(col_name).schema.fields[0].dataType.simpleString() in [\"int\", \"double\", \"float\"]:\n",
    "                # Compute Q1, Q3, and IQR using Spark SQL functions\n",
    "                Q1 = df.approxQuantile(col_name, [0.25], 0.01)[0]\n",
    "                Q3 = df.approxQuantile(col_name, [0.75], 0.01)[0]\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "                # Count outliers\n",
    "                outlier_count = df.filter((col(col_name) < lower_bound) | (col(col_name) > upper_bound)).count()\n",
    "                outliers[col_name] = outlier_count\n",
    "\n",
    "    elif method == \"zscore\":\n",
    "        for col_name in df.columns:\n",
    "            if df.select(col_name).schema.fields[0].dataType.simpleString() in [\"int\", \"double\", \"float\"]:\n",
    "                stats = df.select(\n",
    "                    mean(col(col_name)).alias(\"mean\"),\n",
    "                    stddev(col(col_name)).alias(\"stddev\")\n",
    "                ).collect()[0]\n",
    "\n",
    "                mean_val, std_val = stats[\"mean\"], stats[\"stddev\"]\n",
    "                \n",
    "                # Count outliers\n",
    "                outlier_count = df.filter(((col(col_name) - mean_val) / std_val).abs() > threshold).count()\n",
    "                outliers[col_name] = outlier_count\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'iqr' or 'zscore'.\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "def calculate_outlier_percentage(outliers_dict, total_rows):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of outliers for each column.\n",
    "    \"\"\"\n",
    "    outlier_percentage = {col: (count / total_rows) * 100 for col, count in outliers_dict.items()}\n",
    "    return outlier_percentage\n",
    "\n",
    "def clean_or_winsorize(df, outliers_dict, threshold=5):\n",
    "    \"\"\"\n",
    "    Clean or apply Winsorization based on outlier percentage.\n",
    "\n",
    "    Parameters:\n",
    "        df (Spark DataFrame): Input DataFrame.\n",
    "        outliers_dict (dict): Dictionary with outlier counts.\n",
    "        threshold (int): Percentage threshold to decide action.\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    outlier_percentage = calculate_outlier_percentage(outliers_dict, total_rows)\n",
    "\n",
    "    for col_name, percentage in outlier_percentage.items():\n",
    "        if percentage <= threshold:\n",
    "            # Remove outliers\n",
    "            Q1 = df.approxQuantile(col_name, [0.25], 0.01)[0]\n",
    "            Q3 = df.approxQuantile(col_name, [0.75], 0.01)[0]\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            df = df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))\n",
    "            print(f\"Removed {percentage:.2f}% outliers from column '{col_name}'.\")\n",
    "        else:\n",
    "            # Apply Winsorization using percentile approximation\n",
    "            lower_bound = df.approxQuantile(col_name, [0.05], 0.01)[0]\n",
    "            upper_bound = df.approxQuantile(col_name, [0.95], 0.01)[0]\n",
    "\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                when(col(col_name) < lower_bound, lower_bound)\n",
    "                .when(col(col_name) > upper_bound, upper_bound)\n",
    "                .otherwise(col(col_name))\n",
    "            )\n",
    "            print(f\"Applied Winsorization to column '{col_name}' with {percentage:.2f}% outliers.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example Usage:\n",
    "# Load dataset from BigQuery or CSV\n",
    "# df = spark.read.format(\"bigquery\").option(\"table\", \"your_project.your_dataset.your_table\").load()\n",
    "# df = spark.read.csv(\"your_large_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "total_rows = df.count()\n",
    "outliers_dict = detect_outliers_large(df, method=\"iqr\", threshold=1.5)\n",
    "\n",
    "# Process data based on outlier percentage\n",
    "df_processed = clean_or_winsorize(df, outliers_dict, threshold=5)\n",
    "\n",
    "# Save the processed dataset\n",
    "# df_processed.write.csv(\"cleaned_dataset.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b333ba4e-2e55-4101-babb-e8f60d615fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/04 09:39:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping column 'string_field_20' (only 0 unique value).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Compute unique counts for each column\n",
    "unique_counts_df = (\n",
    "    df.select([countDistinct(col(c)).alias(c) for c in df.columns])\n",
    "    .toPandas()\n",
    "    .melt(var_name=\"Column\", value_name=\"Unique_Values\")\n",
    ")\n",
    "\n",
    "# Convert Pandas DataFrame to PySpark DataFrame with explicit schema\n",
    "unique_counts_spark_df = spark.createDataFrame(\n",
    "    unique_counts_df,\n",
    "    schema=StructType([\n",
    "        StructField(\"Column\", StringType(), True),\n",
    "        StructField(\"Unique_Values\", IntegerType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Ensure 'column_types_df' exists and join with unique counts\n",
    "if \"column_types_df\" in locals():\n",
    "    column_info = column_types_df.join(unique_counts_spark_df, on=\"Column\")\n",
    "else:\n",
    "    raise ValueError(\"column_types_df is not defined. Ensure you run 'identify_column_type(df)' first.\")\n",
    "\n",
    "# Initialize transformation lists\n",
    "indexers, encoders, imputers = [], [], []\n",
    "\n",
    "# Process each column's metadata\n",
    "for row in column_info.rdd.collect():  \n",
    "    col_name = row[\"Column\"]\n",
    "    col_type = row[\"Type\"]\n",
    "    unique_values = row[\"Unique_Values\"]\n",
    "\n",
    "    if col_type in [\"Categorical\", \"Boolean\"]:\n",
    "        if unique_values <= 1:\n",
    "            print(f\"Skipping column '{col_name}' (only {unique_values} unique value).\")\n",
    "            continue\n",
    "        \n",
    "        indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
    "        indexers.append(indexer)\n",
    "        \n",
    "        if unique_values <= 10:\n",
    "            encoder = OneHotEncoder(inputCol=f\"{col_name}_index\", outputCol=f\"{col_name}_encoded\")\n",
    "            encoders.append(encoder)\n",
    "\n",
    "    elif col_type == \"Numerical\":\n",
    "        imputer = Imputer(inputCols=[col_name], outputCols=[col_name + \"_imputed\"], strategy=\"median\")\n",
    "        imputers.append(imputer)\n",
    "\n",
    "# Create and apply the transformation pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + imputers)\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Show transformed dataset\n",
    "#df_transformed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d11c6d-0299-4860-bcd8-376a3a3493b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_UNIQUE_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>...</th>\n",
       "      <th>DISTANCE_imputed</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY_imputed</th>\n",
       "      <th>NAS_DELAY_imputed</th>\n",
       "      <th>OP_CARRIER_FL_NUM_imputed</th>\n",
       "      <th>SECURITY_DELAY_imputed</th>\n",
       "      <th>TAXI_IN_imputed</th>\n",
       "      <th>TAXI_OUT_imputed</th>\n",
       "      <th>WEATHER_DELAY_imputed</th>\n",
       "      <th>WHEELS_OFF_imputed</th>\n",
       "      <th>WHEELS_ON_imputed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1341</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-05</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>1028.00</td>\n",
       "      <td>-17.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1033.00</td>\n",
       "      <td>1042.00</td>\n",
       "      <td>...</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1033</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>AS</td>\n",
       "      <td>64</td>\n",
       "      <td>PSG</td>\n",
       "      <td>WRG</td>\n",
       "      <td>1504.00</td>\n",
       "      <td>-16.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1508.00</td>\n",
       "      <td>1517.00</td>\n",
       "      <td>...</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1508</td>\n",
       "      <td>1517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-06</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>1008.00</td>\n",
       "      <td>-22.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1016.00</td>\n",
       "      <td>1025.00</td>\n",
       "      <td>...</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1016</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>AS</td>\n",
       "      <td>65</td>\n",
       "      <td>WRG</td>\n",
       "      <td>PSG</td>\n",
       "      <td>1031.00</td>\n",
       "      <td>-14.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1035.00</td>\n",
       "      <td>1045.00</td>\n",
       "      <td>...</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1035</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FL_DATE OP_UNIQUE_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  DEP_TIME  \\\n",
       "0 2019-03-05                AS                 65    WRG  PSG       NaN   \n",
       "1 2019-05-05                AS                 65    WRG  PSG   1028.00   \n",
       "2 2019-01-14                AS                 64    PSG  WRG   1504.00   \n",
       "3 2019-07-06                AS                 65    WRG  PSG   1008.00   \n",
       "4 2019-05-13                AS                 65    WRG  PSG   1031.00   \n",
       "\n",
       "   DEP_DELAY  TAXI_OUT  WHEELS_OFF  WHEELS_ON  ...  DISTANCE_imputed  \\\n",
       "0        NaN       NaN         NaN        NaN  ...             31.00   \n",
       "1     -17.00      5.00     1033.00    1042.00  ...             31.00   \n",
       "2     -16.00      4.00     1508.00    1517.00  ...             31.00   \n",
       "3     -22.00      8.00     1016.00    1025.00  ...             31.00   \n",
       "4     -14.00      4.00     1035.00    1045.00  ...             31.00   \n",
       "\n",
       "   LATE_AIRCRAFT_DELAY_imputed  NAS_DELAY_imputed  OP_CARRIER_FL_NUM_imputed  \\\n",
       "0                         3.00               2.00                         65   \n",
       "1                         3.00               2.00                         65   \n",
       "2                         3.00               2.00                         64   \n",
       "3                         3.00               2.00                         65   \n",
       "4                         3.00               2.00                         65   \n",
       "\n",
       "   SECURITY_DELAY_imputed  TAXI_IN_imputed  TAXI_OUT_imputed  \\\n",
       "0                    0.00             6.00             15.00   \n",
       "1                    0.00             3.00              5.00   \n",
       "2                    0.00             4.00              4.00   \n",
       "3                    0.00             4.00              8.00   \n",
       "4                    0.00             2.00              4.00   \n",
       "\n",
       "   WEATHER_DELAY_imputed  WHEELS_OFF_imputed  WHEELS_ON_imputed  \n",
       "0                   0.00                1341               1501  \n",
       "1                   0.00                1033               1042  \n",
       "2                   0.00                1508               1517  \n",
       "3                   0.00                1016               1025  \n",
       "4                   0.00                1035               1045  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformed.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d23bc0a-0491-4dcb-b7c7-baa88f9a14c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_ITERABLE] Column is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m         df_transformed \u001b[38;5;241m=\u001b[39m df_transformed\u001b[38;5;241m.\u001b[39mdrop(col_name)\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_imputed\u001b[39m\u001b[38;5;124m\"\u001b[39m, col_name)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Step 12: Verify That No Nulls Remain\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df_transformed\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m---> 11\u001b[0m     [\u001b[38;5;28msum\u001b[39m(col(c)\u001b[38;5;241m.\u001b[39misNull()\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39malias(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_transformed\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         df_transformed \u001b[38;5;241m=\u001b[39m df_transformed\u001b[38;5;241m.\u001b[39mdrop(col_name)\u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_imputed\u001b[39m\u001b[38;5;124m\"\u001b[39m, col_name)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Step 12: Verify That No Nulls Remain\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df_transformed\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m---> 11\u001b[0m     [\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df_transformed\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/envs/tensorflow/lib/python3.10/site-packages/pyspark/sql/column.py:718\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    719\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_ITERABLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjectName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    720\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_ITERABLE] Column is not iterable."
     ]
    }
   ],
   "source": [
    "# ✅ Step 11: Replace Original Columns with Imputed Versions\n",
    "for row in column_info.collect():\n",
    "    col_name = row[\"Column\"]\n",
    "    col_type = row[\"Type\"]\n",
    "    \n",
    "    if col_type == \"Numerical\":\n",
    "        df_transformed = df_transformed.drop(col_name).withColumnRenamed(f\"{col_name}_imputed\", col_name)\n",
    "\n",
    "# ✅ Step 12: Verify That No Nulls Remain\n",
    "df_transformed.select(\n",
    "    [sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_transformed.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57377f48-532c-44f9-9d6a-622629d7c966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
